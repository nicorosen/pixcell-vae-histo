{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62fca1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Initializing models...\n",
      "\n",
      "Error during execution: Failed to import diffusers.models.autoencoder_kl because of the following error (look up to see its traceback):\n",
      "cannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' (/opt/miniconda3/envs/pixcell-vae-env/lib/python3.10/site-packages/huggingface_hub/__init__.py)\n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/pixcell-vae-env/lib/python3.10/site-packages/diffusers/utils/import_utils.py\", line 684, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"/opt/miniconda3/envs/pixcell-vae-env/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/opt/miniconda3/envs/pixcell-vae-env/lib/python3.10/site-packages/diffusers/models/autoencoder_kl.py\", line 21, in <module>\n",
      "    from ..loaders import FromOriginalVAEMixin\n",
      "  File \"/opt/miniconda3/envs/pixcell-vae-env/lib/python3.10/site-packages/diffusers/loaders.py\", line 28, in <module>\n",
      "    from .models.modeling_utils import _LOW_CPU_MEM_USAGE_DEFAULT, load_model_dict_into_meta\n",
      "  File \"/opt/miniconda3/envs/pixcell-vae-env/lib/python3.10/site-packages/diffusers/models/modeling_utils.py\", line 57, in <module>\n",
      "    import accelerate\n",
      "  File \"/opt/miniconda3/envs/pixcell-vae-env/lib/python3.10/site-packages/accelerate/__init__.py\", line 16, in <module>\n",
      "    from .accelerator import Accelerator\n",
      "  File \"/opt/miniconda3/envs/pixcell-vae-env/lib/python3.10/site-packages/accelerate/accelerator.py\", line 34, in <module>\n",
      "    from huggingface_hub import split_torch_state_dict_into_shards\n",
      "ImportError: cannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' (/opt/miniconda3/envs/pixcell-vae-env/lib/python3.10/site-packages/huggingface_hub/__init__.py)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/x9/3rzbq0tn1cl9z943zndqlfhw0000gn/T/ipykernel_36934/3169766504.py\", line 184, in main\n",
      "    pipeline, uni_model, transform = load_models()\n",
      "  File \"/var/folders/x9/3rzbq0tn1cl9z943zndqlfhw0000gn/T/ipykernel_36934/3169766504.py\", line 47, in load_models\n",
      "    from diffusers import AutoencoderKL, DiffusionPipeline\n",
      "  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\n",
      "  File \"/opt/miniconda3/envs/pixcell-vae-env/lib/python3.10/site-packages/diffusers/utils/import_utils.py\", line 675, in __getattr__\n",
      "    value = getattr(module, name)\n",
      "  File \"/opt/miniconda3/envs/pixcell-vae-env/lib/python3.10/site-packages/diffusers/utils/import_utils.py\", line 674, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"/opt/miniconda3/envs/pixcell-vae-env/lib/python3.10/site-packages/diffusers/utils/import_utils.py\", line 686, in _get_module\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Failed to import diffusers.models.autoencoder_kl because of the following error (look up to see its traceback):\n",
      "cannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' (/opt/miniconda3/envs/pixcell-vae-env/lib/python3.10/site-packages/huggingface_hub/__init__.py)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\"\"\"\n",
    "Optimized PixCell Inference Script\n",
    "\n",
    "This script provides memory-efficient inference for the PixCell model by:\n",
    "1. Processing one sample at a time\n",
    "2. Using gradient checkpointing\n",
    "3. Managing GPU memory with cache clearing\n",
    "4. Using torch.no_grad() for inference\n",
    "\"\"\"\n",
    "\n",
    "# Optional: lower MPS high watermark to release memory more aggressively (harmless on non-MPS)\n",
    "os.environ.setdefault(\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\", \"0.0\")\n",
    "\n",
    "# Standard library imports\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "import argparse\n",
    "\n",
    "DEBUG = os.getenv(\"PIX_DEBUG\", \"0\").strip() == \"1\"\n",
    "\n",
    "# Third-party imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from diffusers import AutoencoderKL, DiffusionPipeline\n",
    "import timm\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login, hf_hub_download\n",
    "import einops\n",
    "\n",
    "# Load environment variables from .env file\n",
    "env_path = Path(__file__).parent.parent / '.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "SEED = 34\n",
    "MODEL_CONFIG = {\n",
    "    'vae_path': \"stabilityai/stable-diffusion-3.5-large\",\n",
    "    'pipeline_path': \"StonyBrook-CVLab/PixCell-1024\",\n",
    "    'pipeline_name': \"StonyBrook-CVLab/PixCell-pipeline\",\n",
    "    'uni_model_name': \"hf-hub:MahmoodLab/UNI2-h\",\n",
    "    'uni_model_config': {\n",
    "        'img_size': 224,\n",
    "        'patch_size': 14,\n",
    "        'depth': 24,\n",
    "        'num_heads': 24,\n",
    "        'init_values': 1e-5,\n",
    "        'embed_dim': 1536,\n",
    "        'mlp_ratio': 2.66667 * 2,\n",
    "        'num_classes': 0,\n",
    "        'no_embed_class': True,\n",
    "        'mlp_layer': timm.layers.SwiGLUPacked,\n",
    "        'act_layer': torch.nn.SiLU,\n",
    "        'reg_tokens': 8,\n",
    "        'dynamic_img_size': True\n",
    "    },\n",
    "    'generation': {\n",
    "        'num_inference_steps': 22,\n",
    "        'guidance_scale': 1.5,\n",
    "        'num_samples': 2\n",
    "    }\n",
    "}\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "DTYPE = torch.float16 if device.type == 'cuda' else torch.float32  # MPS/CPU prefer fp32\n",
    "\n",
    "# Prefer the 256px model on MPS/CPU for speed; keep 1024 only on CUDA\n",
    "if device.type != 'cuda' and MODEL_CONFIG.get('pipeline_path', '').endswith('PixCell-1024'):\n",
    "    print(\"MPS/CPU detected: switching to PixCell-256 for faster inference.\")\n",
    "    MODEL_CONFIG['pipeline_path'] = \"StonyBrook-CVLab/PixCell-256\"\n",
    "    # lighter defaults for non-CUDA\n",
    "    MODEL_CONFIG['generation']['num_inference_steps'] = min(MODEL_CONFIG['generation']['num_inference_steps'], 18)\n",
    "    MODEL_CONFIG['generation']['guidance_scale'] = 1.0\n",
    "\n",
    "# Output directory\n",
    "OUT_DIR = Path(\"generated_samples\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "class ModelLoadingError(Exception):\n",
    "    \"\"\"Custom exception for model loading errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU/CPU cache if needed.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    elif torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "\n",
    "def _load_vae():\n",
    "    print(\"Loading VAE...\")\n",
    "    vae_id = MODEL_CONFIG['vae_path']\n",
    "    try:\n",
    "        print(f\"VAE repo: {vae_id}\")\n",
    "        if \"stable-diffusion-3.5-large\" in vae_id:\n",
    "            vae = AutoencoderKL.from_pretrained(vae_id, subfolder=\"vae\", torch_dtype=DTYPE)\n",
    "        else:\n",
    "            vae = AutoencoderKL.from_pretrained(vae_id, torch_dtype=DTYPE)\n",
    "    except Exception as e:\n",
    "        print(f\"VAE load failed for '{vae_id}' ({e}). Falling back to 'StonyBrook-CVLab/sd-vae-ft-ema-path' (4-ch).\")\n",
    "        vae = AutoencoderKL.from_pretrained(\"StonyBrook-CVLab/sd-vae-ft-ema-path\", torch_dtype=DTYPE)\n",
    "\n",
    "    # Report latent channels so we can quickly spot 4-ch vs 16-ch\n",
    "    try:\n",
    "        lc = getattr(vae.config, 'latent_channels', None)\n",
    "        print(f\"VAE latent_channels: {lc}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    return vae\n",
    "\n",
    "\n",
    "def _load_pipeline(vae):\n",
    "    print(\"Loading PixCell pipeline...\")\n",
    "    pipeline = DiffusionPipeline.from_pretrained(\n",
    "        MODEL_CONFIG['pipeline_path'],\n",
    "        vae=vae,\n",
    "        custom_pipeline=MODEL_CONFIG['pipeline_name'],\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=DTYPE,\n",
    "    )\n",
    "    # Memory saving knobs\n",
    "    try:\n",
    "        pipeline.enable_attention_slicing()\n",
    "        pipeline.enable_vae_slicing()\n",
    "        if device.type == 'cuda':\n",
    "            pipeline.enable_sequential_cpu_offload()\n",
    "        else:\n",
    "            pipeline.to(device, dtype=DTYPE)\n",
    "    except Exception:\n",
    "        pipeline.to(device, dtype=DTYPE)\n",
    "    # Patch: some VAEs have shift_factor=None\n",
    "    if getattr(pipeline.vae.config, 'shift_factor', None) is None:\n",
    "        pipeline.vae.config.shift_factor = 0.0\n",
    "    pipeline.vae.to(device, dtype=DTYPE)\n",
    "    # Sanity: warn if VAE latent_channels != 16 (PixCell uses SD3-family 16-ch latents)\n",
    "    try:\n",
    "        lc = getattr(pipeline.vae.config, 'latent_channels', None)\n",
    "        if lc is not None and lc != 16:\n",
    "            print(f\"[WARN] VAE latent_channels={lc}. PixCell expects 16 (SD3 VAE). You may hit decode errors.\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Ensure components are on the intended device/dtype for MPS/CPU\n",
    "    if device.type != 'cuda':\n",
    "        pipeline.to(device, dtype=DTYPE)\n",
    "    print(\"PixCell pipeline loaded successfully\")\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def _load_uni_model():\n",
    "    \"\"\"Load and return the UNI model and its transform.\"\"\"\n",
    "    print(\"Loading UNI model...\")\n",
    "    uni_model = timm.create_model(\n",
    "        MODEL_CONFIG['uni_model_name'],\n",
    "        pretrained=True,\n",
    "        **MODEL_CONFIG['uni_model_config']\n",
    "    )\n",
    "    uni_model.eval()\n",
    "    uni_model.to(device)\n",
    "    transform = create_transform(**resolve_data_config(uni_model.pretrained_cfg, model=uni_model))\n",
    "    return uni_model, transform\n",
    "\n",
    "\n",
    "def load_models():\n",
    "    \"\"\"Load all required models and return them in a dictionary.\"\"\"\n",
    "    # Accept either HUGGING_FACE_HUB_TOKEN or HF_TOKEN from .env / environment\n",
    "    token = os.getenv('HUGGING_FACE_HUB_TOKEN') or os.getenv('HF_TOKEN')\n",
    "    if not token:\n",
    "        raise ModelLoadingError(\n",
    "            \"Hugging Face token not found. Set HUGGING_FACE_HUB_TOKEN (preferred) or HF_TOKEN in your .env.\"\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        # Login to Hugging Face\n",
    "        login(token=token)\n",
    "        \n",
    "        # Load models\n",
    "        vae = _load_vae()\n",
    "        pipeline = _load_pipeline(vae)\n",
    "        uni_model, transform = _load_uni_model()\n",
    "        \n",
    "        # Get model dimensions\n",
    "        caption_num_tokens = pipeline.transformer.config.caption_num_tokens\n",
    "        caption_channels = pipeline.transformer.config.caption_channels\n",
    "        if DEBUG:\n",
    "            print(f\"[DEBUG] caption_num_tokens={caption_num_tokens}, caption_channels={caption_channels}\")\n",
    "        # Optional override for experiments\n",
    "        override_tokens = None\n",
    "        try:\n",
    "            import sys\n",
    "            for i, a in enumerate(sys.argv):\n",
    "                if a == \"--tokens\" and i+1 < len(sys.argv):\n",
    "                    override_tokens = int(sys.argv[i+1])\n",
    "                    break\n",
    "        except Exception:\n",
    "            pass\n",
    "        if override_tokens is not None:\n",
    "            print(f\"[DEBUG] Overriding caption_num_tokens -> {override_tokens}\")\n",
    "            pipeline.transformer.config.caption_num_tokens = override_tokens\n",
    "            caption_num_tokens = override_tokens\n",
    "        print(f\"Model expects UNI embeddings with shape: (batch_size, {caption_num_tokens}, {caption_channels})\")\n",
    "        \n",
    "        # Generate unconditional embedding\n",
    "        print(\"Generating unconditional embedding...\")\n",
    "        uncond = pipeline.get_unconditional_embedding(1).to(device)\n",
    "        \n",
    "        return {\n",
    "            'pipeline': pipeline,\n",
    "            'uni_model': uni_model,\n",
    "            'transform': transform,\n",
    "            'uncond': uncond,\n",
    "            'dims': (caption_num_tokens, caption_channels)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise ModelLoadingError(f\"Failed to load models: {e}\") from e\n",
    "\n",
    "# ---------------- UNI conditioning helpers ----------------\n",
    "from glob import glob\n",
    "\n",
    "def _fix_token_count(emb: torch.Tensor, target_tokens: int) -> torch.Tensor:\n",
    "    \"\"\"Ensure the UNI embedding has exactly (B, target_tokens, D). Trim or tile as needed.\"\"\"\n",
    "    B, T, D = emb.shape\n",
    "    if T == target_tokens:\n",
    "        return emb\n",
    "    if T > target_tokens:\n",
    "        return emb[:, :target_tokens, :]\n",
    "    # T < target_tokens: tile to reach target_tokens\n",
    "    reps = (target_tokens + T - 1) // T\n",
    "    emb_tiled = emb.repeat(1, reps, 1)[:, :target_tokens, :]\n",
    "    return emb_tiled\n",
    "\n",
    "def _load_uni_from_dir(transform, tiles_dir: str, max_tiles: int = 16) -> torch.Tensor:\n",
    "    \"\"\"Recursively load up to `max_tiles` images from `tiles_dir`.\n",
    "    - Supports extensions: png, jpg, jpeg, tif, tiff (case-insensitive)\n",
    "    - Searches subfolders\n",
    "    - Skips unreadable / tiny files\n",
    "    Returns a tensor of shape (N, C, H, W).\n",
    "    \"\"\"\n",
    "    allowed = {'.png', '.jpg', '' '.jpeg', '.tif', '.tiff'}\n",
    "    paths = []\n",
    "    for root, _, files in os.walk(tiles_dir):\n",
    "        for fn in files:\n",
    "            ext = os.path.splitext(fn)[1].lower()\n",
    "            if ext in allowed:\n",
    "                paths.append(os.path.join(root, fn))\n",
    "    if not paths:\n",
    "        raise FileNotFoundError(f\"No images found in {tiles_dir} (looked for {sorted(allowed)})\")\n",
    "\n",
    "    # Sort for determinism and cap to max_tiles\n",
    "    paths = sorted(paths)[:max_tiles]\n",
    "    print(f\"Found {len(paths)} conditioning tiles (showing up to {max_tiles}).\")\n",
    "    for p in paths[:5]:\n",
    "        print(f\"  - {p}\")\n",
    "\n",
    "    batch = []\n",
    "    kept = 0\n",
    "    for p in paths:\n",
    "        try:\n",
    "            im = Image.open(p)\n",
    "            im = im.convert(\"RGB\")\n",
    "            # quick size sanity (skip tiny icons by accident)\n",
    "            if min(im.size) < 64:\n",
    "                continue\n",
    "            batch.append(transform(im))\n",
    "            kept += 1\n",
    "        except Exception:\n",
    "            continue\n",
    "    if kept == 0:\n",
    "        raise FileNotFoundError(f\"No readable RGB images found in {tiles_dir} after filtering.\")\n",
    "    return torch.stack(batch, dim=0)\n",
    "\n",
    "def _get_conditioning_batch(transform) -> torch.Tensor:\n",
    "    \"\"\"Return a batch for UNI: CLI --cond_dir > ENV CONDITION_DIR > HF fallback.\"\"\"\n",
    "    # CLI override\n",
    "    import sys\n",
    "    try:\n",
    "        if \"--cond_dir\" in sys.argv:\n",
    "            idx = sys.argv.index(\"--cond_dir\")\n",
    "            if idx+1 < len(sys.argv):\n",
    "                cond_dir = sys.argv[idx+1]\n",
    "            else:\n",
    "                cond_dir = \"\"\n",
    "        else:\n",
    "            cond_dir = os.getenv(\"CONDITION_DIR\", \"\").strip()\n",
    "    except Exception:\n",
    "        cond_dir = os.getenv(\"CONDITION_DIR\", \"\").strip()\n",
    "    if cond_dir:\n",
    "        print(f\"Using local conditioning tiles from: {cond_dir}\")\n",
    "        batch = _load_uni_from_dir(transform, cond_dir, max_tiles=16)\n",
    "        print(f\"Loaded {batch.shape[0]} tiles for UNI conditioning.\")\n",
    "        return batch\n",
    "    else:\n",
    "        print(\"No cond_dir provided; using bundled HF example image for conditioning\")\n",
    "        return _download_and_process_image(transform)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def _download_and_process_image(transform):\n",
    "    \"\"\"Fallback: download and process the example image for conditioning (used if CONDITION_DIR not set).\"\"\"\n",
    "    path = hf_hub_download(\n",
    "        repo_id=MODEL_CONFIG['pipeline_path'],\n",
    "        filename=\"test_image.png\"\n",
    "    )\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    \n",
    "    # Prepare image patches for UNI model\n",
    "    uni_patches = np.array(image)\n",
    "    uni_patches = einops.rearrange(\n",
    "        uni_patches, \n",
    "        '(d1 h) (d2 w) c -> (d1 d2) h w c', \n",
    "        d1=4, d2=4  # Split 1024x1024 into 16x 256x256 patches\n",
    "    )\n",
    "    return torch.stack([transform(Image.fromarray(item)) for item in uni_patches])\n",
    "\n",
    "\n",
    "def _extract_uni_embeddings(uni_model, transform, image_batch: torch.Tensor):\n",
    "    \"\"\"Extract UNI embeddings from a batch of transformed images.\n",
    "    Args:\n",
    "        uni_model: UNI-2h encoder (in eval mode, on device)\n",
    "        transform: (unused here; kept for API compatibility)\n",
    "        image_batch: Tensor of shape (N, C, H, W) already transformed\n",
    "    Returns:\n",
    "        Tensor of shape (1, N, D) on the active device\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        feats = uni_model(image_batch.to(device))  # (N, D)\n",
    "        if feats.dim() == 2:\n",
    "            feats = feats.unsqueeze(0)             # (1, N, D)\n",
    "    return feats.to(device)\n",
    "\n",
    "\n",
    "def _generate_samples_impl(pipeline, uni_emb, uncond, num_samples, timestamp):\n",
    "    \"\"\"Core implementation of sample generation.\"\"\"\n",
    "    samples = []\n",
    "    gen_config = MODEL_CONFIG['generation']\n",
    "    \n",
    "    # Determine device type for autocast\n",
    "    use_autocast = 'cuda' in str(device)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        print(f\"\\nGenerating sample {i+1}/{num_samples}\")\n",
    "        try:\n",
    "            generator = torch.Generator(\"cpu\").manual_seed(SEED + i)\n",
    "            \n",
    "            # NOTE: PixCell will raise: \"Number of UNI embeddings must match the ones used in training (1)\"\n",
    "            # if we pass more than caption_num_tokens tokens. The pooling/trim above guarantees compliance.\n",
    "            # Only use autocast for CUDA devices\n",
    "            if DEBUG:\n",
    "                print(f\"[DEBUG] call: steps={gen_config['num_inference_steps']} guidance={gen_config['guidance_scale']} uni.shape={tuple(uni_emb.shape)} uncond.shape={tuple(uncond.shape)}\")\n",
    "            if use_autocast:\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    sample = pipeline(\n",
    "                        uni_embeds=uni_emb,\n",
    "                        negative_uni_embeds=uncond,\n",
    "                        guidance_scale=gen_config['guidance_scale'],\n",
    "                        generator=generator,\n",
    "                        num_inference_steps=gen_config['num_inference_steps']\n",
    "                    )\n",
    "            else:\n",
    "                # For MPS/CPU, run without autocast\n",
    "                sample = pipeline(\n",
    "                    uni_embeds=uni_emb,\n",
    "                    negative_uni_embeds=uncond,\n",
    "                    guidance_scale=gen_config['guidance_scale'],\n",
    "                    generator=generator,\n",
    "                    num_inference_steps=gen_config['num_inference_steps']\n",
    "                )\n",
    "            \n",
    "            # Save the generated image\n",
    "            img_path = OUT_DIR / f'sample_{timestamp}_{i:02d}.png'\n",
    "            sample.images[0].save(img_path)\n",
    "            print(f\"Saved to {img_path}\")\n",
    "            samples.append(sample.images[0])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating sample {i+1}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    return samples\n",
    "\n",
    "\n",
    "def generate_samples(pipeline, num_samples=None, uni_model=None, transform=None, uncond=None):\n",
    "    \"\"\"\n",
    "    Generate samples using the PixCell pipeline with UNI model.\n",
    "    \n",
    "    Args:\n",
    "        pipeline: Loaded PixCell pipeline\n",
    "        num_samples: Number of samples to generate (uses config if None)\n",
    "        uni_model: Loaded UNI model\n",
    "        transform: Image transform for UNI model\n",
    "        uncond: Unconditional embedding for guidance\n",
    "    \"\"\"\n",
    "    if None in [pipeline, uni_model, transform, uncond]:\n",
    "        raise ValueError(\"All model components must be provided\")\n",
    "    \n",
    "    print(\"\\nStarting generation...\")\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    num_samples = num_samples or MODEL_CONFIG['generation']['num_samples']\n",
    "    \n",
    "    try:\n",
    "        # Build conditioning batch (local tiles preferred via CONDITION_DIR)\n",
    "        image_input = _get_conditioning_batch(transform)\n",
    "        print(f\"Conditioning batch: {tuple(image_input.shape)}\")\n",
    "\n",
    "        # Extract UNI embeddings\n",
    "        uni_emb = _extract_uni_embeddings(uni_model, transform, image_input)  # (1, T, D)\n",
    "\n",
    "        if DEBUG:\n",
    "            print(f\"[DEBUG] uni_emb raw shape: {tuple(uni_emb.shape)} | mean={uni_emb.mean().item():.4f} std={uni_emb.std().item():.4f}\")\n",
    "\n",
    "        # Match PixCell's expected caption token count\n",
    "        target_tokens = getattr(pipeline.transformer.config, 'caption_num_tokens', 1)\n",
    "        if uni_emb.shape[1] != target_tokens:\n",
    "            if target_tokens == 1:\n",
    "                # Average-pool tokens to a single conditioning token\n",
    "                uni_emb = uni_emb.mean(dim=1, keepdim=True)  # (1, 1, D)\n",
    "            else:\n",
    "                # Trim or tile to the required number of tokens\n",
    "                uni_emb = _fix_token_count(uni_emb, target_tokens)\n",
    "        print(f\"UNI embeddings shaped for PixCell: {tuple(uni_emb.shape)} (target tokens={target_tokens})\")\n",
    "\n",
    "        # Get matching-size unconditional embedding for classifier-free guidance\n",
    "        uncond = pipeline.get_unconditional_embedding(uni_emb.shape[0]).to(device)\n",
    "\n",
    "        return _generate_samples_impl(pipeline, uni_emb, uncond, num_samples, timestamp)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during sample generation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "\n",
    "# ----------- Helper for unconditional control image -----------\n",
    "def run_uncond_control(pipeline, uncond, timestamp):\n",
    "    gen_config = MODEL_CONFIG['generation']\n",
    "    generator = torch.Generator(\"cpu\").manual_seed(SEED + 999)\n",
    "    sample = pipeline(\n",
    "        uni_embeds=uncond,               # <- use uncond as the conditioner\n",
    "        negative_uni_embeds=uncond,\n",
    "        guidance_scale=gen_config['guidance_scale'],\n",
    "        generator=generator,\n",
    "        num_inference_steps=gen_config['num_inference_steps']\n",
    "    )\n",
    "    img_path = OUT_DIR / f'control_uncond_{timestamp}.png'\n",
    "    sample.images[0].save(img_path)\n",
    "    print(f\"Saved uncond-control to {img_path}\")\n",
    "\n",
    "def display_samples(samples):\n",
    "    \"\"\"Display generated samples in a grid.\"\"\"\n",
    "    if not samples:\n",
    "        print(\"No samples to display\")\n",
    "        return\n",
    "        \n",
    "    n = len(samples)\n",
    "    fig, axes = plt.subplots(1, n, figsize=(n * 4, 4))\n",
    "    \n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "        \n",
    "    for i, img in enumerate(samples):\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f'Sample {i+1}')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument(\"--cond_dir\", type=str, default=os.getenv(\"CONDITION_DIR\", \"\"))\n",
    "    p.add_argument(\"--steps\", type=int, default=None)\n",
    "    p.add_argument(\"--guidance\", type=float, default=None)\n",
    "    p.add_argument(\"--tokens\", type=int, default=None, help=\"Override caption_num_tokens (debug)\")\n",
    "    p.add_argument(\"--uncond_test\", action=\"store_true\", help=\"Run a control sample using unconditional embedding as the conditioner\")\n",
    "    p.add_argument(\"--fast\", action=\"store_true\", help=\"Use very light settings (steps=12, guidance=1.0)\")\n",
    "    return p.parse_args()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the PixCell inference pipeline.\"\"\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    args = parse_args()\n",
    "    if args.fast:\n",
    "        MODEL_CONFIG['generation']['num_inference_steps'] = 12\n",
    "        MODEL_CONFIG['generation']['guidance_scale'] = 1.0\n",
    "    if args.steps is not None:\n",
    "        MODEL_CONFIG['generation']['num_inference_steps'] = int(args.steps)\n",
    "    if args.guidance is not None:\n",
    "        MODEL_CONFIG['generation']['guidance_scale'] = float(args.guidance)\n",
    "\n",
    "    # Echo which token env is set (helps debug auth issues)\n",
    "    if os.getenv('HUGGING_FACE_HUB_TOKEN'):\n",
    "        print(\"Auth: using HUGGING_FACE_HUB_TOKEN from environment/.env\")\n",
    "    elif os.getenv('HF_TOKEN'):\n",
    "        print(\"Auth: using HF_TOKEN from environment/.env\")\n",
    "\n",
    "    try:\n",
    "        # Load models\n",
    "        print(\"Initializing models...\")\n",
    "        models = load_models()\n",
    "\n",
    "        # Set conditioning dir from CLI arg if provided\n",
    "        cond_dir = args.cond_dir.strip() if args.cond_dir else os.getenv(\"CONDITION_DIR\", \"\").strip()\n",
    "        if cond_dir:\n",
    "            print(f\"Conditioning directory set to: {cond_dir}\")\n",
    "            os.environ[\"CONDITION_DIR\"] = cond_dir\n",
    "\n",
    "        # Generate samples\n",
    "        print(\"\\nGenerating samples...\")\n",
    "        samples = generate_samples(\n",
    "            pipeline=models['pipeline'],\n",
    "            num_samples=MODEL_CONFIG['generation']['num_samples'],\n",
    "            uni_model=models['uni_model'],\n",
    "            transform=models['transform'],\n",
    "            uncond=models['uncond']\n",
    "        )\n",
    "\n",
    "        # Optionally run unconditional control branch\n",
    "        if args.uncond_test:\n",
    "            run_uncond_control(models['pipeline'], models['uncond'], datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
    "\n",
    "        # Display results\n",
    "        if samples:\n",
    "            print(f\"\\nSuccessfully generated {len(samples)} samples\")\n",
    "            display_samples(samples)\n",
    "        else:\n",
    "            print(\"\\nNo samples were generated.\")\n",
    "\n",
    "    except ModelLoadingError as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "        print(\"Please check your Hugging Face token and internet connection.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        # Clean up\n",
    "        if 'models' in locals() and 'pipeline' in models:\n",
    "            del models['pipeline']\n",
    "        clear_memory()\n",
    "        print(\"\\nDone!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717529a6",
   "metadata": {},
   "source": [
    "Next: Proceed to dataset prep and reconstructions: [02_dataset_prep.ipynb](02_dataset_prep.ipynb:1)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pixcell-vae)",
   "language": "python",
   "name": "pixcell-vae-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
